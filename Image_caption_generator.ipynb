{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image caption generator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d557810df1254cd7889d0dd41c61c57b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eee9fbbcf6ff443d9c79c5542c1a79a0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_06f38d6ed4654f5f8075d32403ce3245",
              "IPY_MODEL_ebd40bc96c714ab4a83e66ebc0c46671"
            ]
          }
        },
        "eee9fbbcf6ff443d9c79c5542c1a79a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "06f38d6ed4654f5f8075d32403ce3245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_63be0503b37f4e5793f2de16054728db",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 108857766,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 108857766,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_80e56450d14247ef9cf318c1f4397601"
          }
        },
        "ebd40bc96c714ab4a83e66ebc0c46671": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7c368498f5b9473197766265874af96d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 104M/104M [16:13&lt;00:00, 112kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f32fe0813d1b466e8e8bbce006ac1d0c"
          }
        },
        "63be0503b37f4e5793f2de16054728db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "80e56450d14247ef9cf318c1f4397601": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7c368498f5b9473197766265874af96d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f32fe0813d1b466e8e8bbce006ac1d0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwUKKDIyD4t1"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import spacy\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mKV-nCNHk90",
        "outputId": "2f86f999-1f37-460e-a5ba-b382f6fad79f"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile(\"/content/drive/MyDrive/flickr8k.zip\",\"r\") as zip:\n",
        "  zip.extractall()\n",
        "  print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xky35gEiHsrR"
      },
      "source": [
        "spacy_eng = spacy.load(\"en\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPr1PYKtJvAr"
      },
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.itos = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    \n",
        "    def tokenizer_eng(self, text):\n",
        "        self.text = text\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(self.text)]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "                \n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "        return [\n",
        "                self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "                for token in tokenized_text\n",
        "        ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VkVTxAMKeVI"
      },
      "source": [
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, root_dir, captions_file, transform = None, freq_threshold = 5):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(captions_file)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get img, caption columns\n",
        "        self.images = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "\n",
        "        # Initialize vocabulary and build vocab\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocabulary(self.captions.tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        caption = self.captions[index]\n",
        "        image_id = self.images[index]\n",
        "        image = Image.open(os.path.join(self.root_dir, image_id)).convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        numericalized_caption += self.vocab.numericalize(caption)\n",
        "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "        return image, torch.tensor(numericalized_caption)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khnhg_-fMfOY"
      },
      "source": [
        "class MyCollate:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "        targets = [item[1] for item in batch]\n",
        "        targets = pad_sequence(targets, batch_first=False, padding_value = self.pad_idx)\n",
        "\n",
        "        return imgs, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcov9PxWO55t"
      },
      "source": [
        "transform = transforms.Compose([transforms.Resize((356, 356)),\n",
        "                                      transforms.RandomCrop((299, 299)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCu_j4AJQAIH"
      },
      "source": [
        "data_loc = \"/content/images\"  #/content/images\n",
        "captions_loc = \"/content/captions.txt\" #/content/captions.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPVngQl8QGo-"
      },
      "source": [
        "data = FlickrDataset(root_dir=data_loc,\n",
        "                     captions_file = captions_loc,\n",
        "                     transform = transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgZ2YZP8SKZw"
      },
      "source": [
        "pad_idx = data.vocab.stoi[\"<PAD>\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BQYQjgtQcYm"
      },
      "source": [
        "data_loader = DataLoader(data, batch_size = 32, shuffle=True, num_workers=2, collate_fn=MyCollate(pad_idx = pad_idx), pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT6ZMPTDSFl6"
      },
      "source": [
        "import torchvision.models as models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLlT-Ue83F3K"
      },
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size, train_CNN=False):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        self.train_CNN = train_CNN\n",
        "        self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
        "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.inception(images)\n",
        "\n",
        "        # To change the weights or fine tune the last layer\n",
        "        for name, param in self.inception.named_parameters():\n",
        "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
        "                param.requires_grad = True\n",
        "            else:\n",
        "                param.requires_grad = self.train_CNN\n",
        "        \n",
        "        return self.dropout(self.relu(features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "427u5pwR5K9L"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.dropout(self.embed(captions))\n",
        "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23dJbahy6jmm"
      },
      "source": [
        "class CNNtoRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocan_size, num_layers):\n",
        "        super(CNNtoRNN, self).__init__()\n",
        "        self.encoderCNN = EncoderCNN(embed_size)\n",
        "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoderCNN(images)\n",
        "        outputs = self.decoderRNN(features, captions)\n",
        "        return outputs\n",
        "\n",
        "    def caption_image(self, image, vocabulary, max_length = 50):\n",
        "        result_caption = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x = self.encoderCNN(image).unsqueeze(0)\n",
        "            states = None\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
        "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
        "                predicted = output.argmax(1)\n",
        "\n",
        "                result_caption.append(predicted.item())\n",
        "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
        "\n",
        "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
        "                    break\n",
        "\n",
        "        return [vocabulary.itos[idx] for idx in result_caption]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMLMgHaL8PeP"
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-npfndL9Xkf"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlVmftnL9uG9"
      },
      "source": [
        "embed_size = 256\n",
        "hidden_size = 256\n",
        "vocab_size = len(data.vocab)\n",
        "num_layers = 1\n",
        "lr = 2e-4\n",
        "num_epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "d557810df1254cd7889d0dd41c61c57b",
            "eee9fbbcf6ff443d9c79c5542c1a79a0",
            "06f38d6ed4654f5f8075d32403ce3245",
            "ebd40bc96c714ab4a83e66ebc0c46671",
            "63be0503b37f4e5793f2de16054728db",
            "80e56450d14247ef9cf318c1f4397601",
            "7c368498f5b9473197766265874af96d",
            "f32fe0813d1b466e8e8bbce006ac1d0c"
          ]
        },
        "id": "GLp06jGw90jO",
        "outputId": "7c2cb85a-5e97-4a3f-a8d9-8c93295de27d"
      },
      "source": [
        "model = CNNtoRNN(embed_size , hidden_size , vocab_size , num_layers).to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d557810df1254cd7889d0dd41c61c57b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=108857766.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rgu-zJ1-DcW"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = data.vocab.stoi[\"<PAD>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr = lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXDbgonk-i42",
        "outputId": "35ec3d4a-790d-4ffa-fa76-db66ac8bcd99"
      },
      "source": [
        "model.train()\n",
        "train_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for idx, (imgs, caption) in enumerate(data_loader):\n",
        "        imgs = imgs.to(device)\n",
        "        caption = caption.to(device)\n",
        "\n",
        "        outputs = model(imgs, caption[:-1])  # Not take <EOS>\n",
        "        loss = criterion(outputs.reshape(-1, outputs.shape[2]), caption.reshape(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward(loss)\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch +1} done!!\")\n",
        "\n",
        "print(f\"Training Complete\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 done!!\n",
            "Epoch 2 done!!\n",
            "Epoch 3 done!!\n",
            "Epoch 4 done!!\n",
            "Epoch 5 done!!\n",
            "Epoch 6 done!!\n",
            "Epoch 7 done!!\n",
            "Epoch 8 done!!\n",
            "Epoch 9 done!!\n",
            "Epoch 10 done!!\n",
            "Epoch 11 done!!\n",
            "Epoch 12 done!!\n",
            "Epoch 13 done!!\n",
            "Epoch 14 done!!\n",
            "Epoch 15 done!!\n",
            "Epoch 16 done!!\n",
            "Epoch 17 done!!\n",
            "Epoch 18 done!!\n",
            "Epoch 19 done!!\n",
            "Epoch 20 done!!\n",
            "Training Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxQhU0jPBHgD"
      },
      "source": [
        "from PIL import Image\n",
        "from IPython.display import Image as im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "W12GVkV55XDK",
        "outputId": "f46009b4-8846-4634-be3b-613f9a1bedc8"
      },
      "source": [
        "im(filename=\"/content/dog_img.jpg\",width = 400, height = 300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/jpeg": {
              "width": 400,
              "height": 300
            }
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TcSBHOt7nfJ",
        "outputId": "ee81269c-5451-4ac0-ecb2-23f5397a2efd"
      },
      "source": [
        "model.eval()\n",
        "test_image_a = transform(Image.open(\"/content/dog_img.jpg\").convert(\"RGB\")).unsqueeze(0)\n",
        "\n",
        "print(\"Example 4 OUPUT: \"\n",
        "+\" \".join(model.caption_image(test_image_a.to(device),data.vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example 4 OUPUT: <SOS> a dog runs through the snow . <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCAY0xS970w7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}